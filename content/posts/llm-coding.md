---
title: "Programmieren mit LLMs, eine Reflexion"
date: 2025-08-14T14:44:05+02:00
draft: false
description: "Wie wirkt sich der regelmäßige Gebrauch von LLMs (Large Language Models) wie ChatGPT oder Gemini auf so etwas banales wie das Programmieren aus? Seit knapp einem halben Jahr habe ich viele Fragen, die ich davor auf Stack Overflow, in Blogs oder in den offiziellen Dokumentationen gesucht habe, mit einem LLM beantwortet. Vor knapp einer Woche habe ich damit aufgehört und frage mich nun: habe ich verlernt, wie man online nach Informationen sucht?"
tags: []
---

Ich habe mich lange Zeit davor gescheut, ein LLM (Large Language Model) zu verwenden. Die ersten öffentlichen Versionen von ChatGPT habe ich noch belächelt und war stolz darauf, keinen Gebrauch davon zu machen. Es hat sich schlichtweg nicht richtig angefühlt, auch, wenn ich damals meine Kritik, oder zumindest meine Einwände dagegen, nicht richtig einordnen konnte. Dennoch habe ich mich irgendwann einmal durchgerungen, ChatGPT auszuprobieren. Die ersten Versuche waren vorsichtig formuliert ... seltsam. Plötzlich mit einem Ding zu kommunizieren, das auf Fragen Antworten geben konnte und mir das Gefühl gab, im Austausch mit jemandem (etwas ?) zu stehen, war äußerst befremdlich. Die ersten Schritte waren vor allem auf die Philosophie bezogen und hatten mit Technik noch wenig zu tun.

Das sollte sich vor einem halben Jahr ändern, als ich ein kleines [kleines Projekt](https://github.com/mfreimueller/christmas-tree-bf) umsetzte. Zugegeben, ChatGPT ist recht schlecht darin, Code in Brainfuck richtig zu verstehen oder sinnvolle Lösungen anzubieten. Aber es half mir dabei, in kleinen Schritten die Sprache zu erlernen. Als ich dann, ein oder zwei Monate später, ein kleines Machine Learning-Projekt in Python umsetzte, war es schon hilfreicher. Und später, bei der Entwicklung [meines brainfuck Compilers](https://github.com/mfreimueller/goo) hat es mir viel Aufwand und Zeit gespart, indem es mir Fehler in meinem Assembler-Code aufdeckte und mir Anregungen gab, wie ich den Compiler umsetzen sollte. Richtigen Code, den mir ein LLM generiert hat, habe ich nie direkt verwendet. Vor IDE-Integrationen von Copilot habe ich immer die Finger gelassen, weil mich solche Tools meines Meinung nach zu sehr vom eigentlichen Prozess des Programmierens entfernen. Das wäre in etwa so, als würde ein Autor mit einem LLM einen Großteil seines Buches generieren lassen. Ob eine Zukunft wünschenswert ist, in der Literatur (um ein Beispiel zu nennen) primär von LLMs generiert wird, überlasse ich jedem selbst zu entscheiden. Ich finde eine solche Zukunft jedenfalls dystopisch. Selbiges gilt für das Programmieren.

Vor einer Woche habe ich dann ein kleines Projekt für meine zukünftige Lehrtätigkeit umgesetzt. Wieder habe ich primär ChatGPT verwendet und nur ein wenig in die offizielle Dokumentation gesehen, wenn der Output von ChatGPT nicht hilfreich war. Aber, als ich am Ende des Projekts angekommen war, ging mir eine Frage durch den Kopf:

Habe ich verlernt, wie man Informationen im Internet sucht?

Vor einigen Jahren, als ich noch professionell als Softwareentwickler gearbeitet hatte, gab es LLMs noch nicht, oder sie steckten noch so weit in den Kinderschuhen, dass sie gänzlich uninteressant waren. Wenn ich Probleme bei meinen Tätigkeiten hatte, gab es nur eine Lösung (abgesehen von Ratschlägen der Kollegen): nach der Antwort suchen, auf Google, auf Blogs oder auf Stack Overflow. Die Suche war ein Prozess, der mitunter eine längere Zeit in Anspruch nahm. Länger jedenfalls, als der jetzige Prozess, einem LLM eine Frage vorzusetzen und dann mit der Antwort zu arbeiten. Aber, und das ist ein entscheidender Punkt, durch die aktive Suche bin ich unweigerlich auf unterschiedliche Lösungsansätze und Meinungen gestoßen. Denn es gibt oft nicht eine einzige Art, ein Problem richtig zu lösen, sondern viele verschiedene, die alle unterschiedliche Vor- und Nachteile haben. Gerade Stack Overflow hat das aufgrund seiner Diskursnatur verdeutlicht \*. Wenn eine Lösung nicht optimal ist, findet man Kommentare, die darauf hinweisen. Gleichzeitig fungiert eine Plattform wie Stack Overflow auch als ein stetig wachsender Speicher von Information, der sich über die Jahre aktualisiert, indem Benutzer alte Threads neu aufgreifen und modernere Lösungen vorschlagen.

Mit einem LLM geht aber genau diese Kultur verloren. Auch das Organische wachsen von Information verschwindet. Vielmehr wird man mit einer Lösung konfrontiert, welche den Anschein (oder Anspruch) der Gültigkeit hat. Man könnte natürlich den Ansatz wählen, unterschiedliche LLMs nach derselben Information zu fragen und dann die Lösungen zu vergleichen, nur widerspricht dieser Ansatz dem scheinbaren Vorteil eines LLMs (und, als weiterer Einwand, liefern selbst gleiche LLMs unterschiedliche Antwort auf dieselben Fragen - fragen mehrere Personen Gemini etwa danach, wie viele Wölfe es in Südtirol ungefähr gibt, liefern die Antworten eine Spannweite des Bestands von bis zu 1.000 Exemplaren \*). Die philosophisch interessante Frage an der Stelle ist, worum es sich bei dem handelt, was ein LLM generiert. Ist es eine bloße Meinung? Denn Wahrheit scheint schon allein aufgrund der Möglichkeit des Halluzinierens (i.e. des Generieren von Fehlinformationen) außen vor zu sein. Aber wer oder was kann eine Meinung haben? Und ist das Bezugnehmen auf Fakten und das Generieren einer Antwort auf Basis von statistischen Prozessen tatsächlich eine Meinung? Oder ist es weniger als eine Meinung? Ist es Information? Oder eher so etwas wie Meta-Information, da sie sich aus vorhandener Information aggregiert?

Von diesen Fragen abgesehen, die ich hier nicht beantworten kann, stellt sich mir die Frage, was LLMs mit unserem Umgang mit Informationen und Wissen tun. Ich habe heute ein [kleines Projekt](https://github.com/mfreimueller/nitch-scrap) geschrieben, für das ich bewusst Abstand von LLMs genommen haben. Zu meiner Überraschung war ich frustriert, als ich nicht sofort eine Lösung auf meine Probleme gefunden habe. Die einzelnen Dokumentationen durchzugehen war langsam und, relativ gesehen, aufwändig. Es hat sich so angefühlt, als hätte ich das Suchen nach Information verlernt, oder vielmehr, als hätte ich mich zu sehr daran gewöhnt, eine quasi Instant-Antwort zu bekommen. Ich habe mich gefragt, ob ich früher wirklich so gearbeitet habe? Es wirkt so fremd und befremdlich.

Das war der Moment, an dem ich für mich beschlossen habe, gänzlich von LLMs Abstand zu nehmen. Nicht, weil ich gegen Technologie oder Neuerungen bin. Sondern, weil mir LLMs mehr genommen haben, als sie mir schließlich zurückgegeben haben. Der Gewinn an Zeit und Einfachheit rechtfertig in meinen Augen nicht den Verlust am kritischen Vergleich und am Prozess des Suchens. Natürlich habe ich schon ein anderes Verständnis vom Prozess des Suchens nach Information, als etwa eine Generation vor mir. Der Gedanke, in eine Bibliothek zu gehen, um nach etwas zu suchen, ist für mich höchstens ein nostalgischer Gedanke an eine Zeit, die ich nie erlebt habe. Für mich ist Suchen fest mit Suchmaschinen verbunden, mit dem Durchwühlen von zahllosen Seiten und Foreneinträgen, auf der Suche nach einer Antwort, oder zumindest einem Hinweis darauf. Und selbst, wenn diese Art des Suchens immer noch wesentlich schneller ist, als das Aufsuchen einer Bibliothek, ist es eine Geschwindigkeit, mit der ich gut arbeiten und leben kann. Womit ich nicht kann, ist die Geschwindigkeit der LLMs. Aber das ist wohl auch ein Grund, warum ich mit TikTok und Instagram/Youtube Reels nicht umgehen kann. Ich finde, dass man im Prozess des aktiven Suchens mehr macht, als einfach nur zu suchen. Man tritt in einen Diskurs ein mit zahllosen anderen Menschen, die ebenfalls auf der Suche nach ähnlichen Antworten sind. Man vergleicht, man hinterfragt kritisch. Manchmal findet man keine Antwort, dann muss man selbst probieren. Und all das wird verunmöglicht, wenn ein LLM selbstbewusst eine Antwort liefert und dadurch alle Alternativen erstickt. Ist es überhaupt möglich, die eine beste Antwort zu generieren, auf Basis statistischer Häufigkeit, oder anderer Metriken?

Vielleicht ist das aber auch nur der Punkt, an dem ich merke, dass ich älter werde. Vielleicht liegt die Zukunft tatsächlich in LLMs und in weiteren Technologien, wie neuralen Implantaten, die einem Antworten liefern, sobald man sich eine Frage stellt. Vielleicht bin ich nicht Transhumanist genug, um das Schöne dieser Zukunft zu sehen.

\* Danke Steffi für die beiden Anregungen, bzw. das Beispiel!